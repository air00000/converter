import asyncio
import io
import logging
import os
import re
import tempfile
import zipfile
from pathlib import Path
from typing import Any, Iterable, List, Mapping, Optional, Tuple
from urllib.parse import parse_qs, unquote, urlparse

import aiohttp
import gdown

from aiogram import Bot, Dispatcher, F
from aiogram.client.default import DefaultBotProperties
from aiogram.enums import ParseMode
from aiogram.exceptions import TelegramBadRequest, TelegramRetryAfter
from aiogram.filters import Command, CommandStart
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types import BufferedInputFile, Message
from dotenv import load_dotenv


MAX_TELEGRAM_FILE_SIZE = 49 * 1024 * 1024

PLACEHOLDER_PATTERN = re.compile(r"\{([a-zA-Z_][\w-]*)\}")
URL_PATTERN = re.compile(r"https?://\S+")


async def _send_document(
    message: Message,
    file_bytes: bytes,
    filename: str,
    caption: str,
) -> bool:
    size = len(file_bytes)
    if size > MAX_TELEGRAM_FILE_SIZE:
        size_mb = size / (1024 * 1024)
        await _answer_with_retry(
            message,
            (
                "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –ø—Ä–µ–≤—ã—à–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Telegram "
                f"({size_mb:.1f} –ú–ë). –ü–æ–ø—Ä–æ–±—É–π —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —á–∞—Å—Ç–∏."
            ),
        )
        return False

    retries_left = 3
    while retries_left > 0:
        try:
            await message.answer_document(
                BufferedInputFile(file_bytes, filename=filename),
                caption=caption,
            )
            return True
        except TelegramRetryAfter as exc:
            retries_left -= 1
            await asyncio.sleep(exc.retry_after + 1)
        except TelegramBadRequest as exc:
            retry_after = _extract_retry_after_seconds(str(exc))
            if retry_after is not None and retries_left > 1:
                retries_left -= 1
                await asyncio.sleep(retry_after + 1)
                continue
            logging.exception("Failed to send document %s", filename, exc_info=exc)
            break
        except (aiohttp.ClientError, ConnectionError) as exc:
            logging.exception("Failed to send document %s", filename, exc_info=exc)
            break

    await _answer_with_retry(
        message,
        "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ —á—É—Ç—å –ø–æ–∑–∂–µ.",
    )
    return False


def _extract_retry_after_seconds(message: str) -> Optional[int]:
    match = re.search(r"retry after (?P<seconds>\d+)", message, re.IGNORECASE)
    if match:
        try:
            return int(match.group("seconds"))
        except ValueError:
            return None
    return None


async def _answer_with_retry(message: Message, text: str, **kwargs: Any) -> None:
    retries_left = 3
    while retries_left > 0:
        try:
            await message.answer(text, **kwargs)
            return
        except TelegramRetryAfter as exc:
            retries_left -= 1
            await asyncio.sleep(exc.retry_after + 1)
        except TelegramBadRequest as exc:
            retry_after = _extract_retry_after_seconds(str(exc))
            if retry_after is not None and retries_left > 1:
                retries_left -= 1
                await asyncio.sleep(retry_after + 1)
                continue
            logging.exception("Failed to send message '%s'", text, exc_info=exc)
            return
        except (aiohttp.ClientError, ConnectionError) as exc:
            logging.exception("Failed to send message '%s'", text, exc_info=exc)
            return

    logging.warning("Failed to send message '%s' after retries", text)


class ConversionStates(StatesGroup):
    input_mask = State()
    output_mask = State()
    lines_per_file = State()
    waiting_file = State()


def _extract_placeholders(mask: str) -> List[str]:
    return [match.group(1) for match in PLACEHOLDER_PATTERN.finditer(mask)]


def _mask_to_regex(mask: str) -> re.Pattern[str]:
    placeholders = list(PLACEHOLDER_PATTERN.finditer(mask))
    if not placeholders:
        raise ValueError("–ú–∞—Å–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –≤ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö.")

    pattern_parts: List[str] = []
    last_index = 0

    for idx, placeholder in enumerate(placeholders):
        pattern_parts.append(re.escape(mask[last_index : placeholder.start()]))
        name = placeholder.group(1)
        quantifier = ".+" if idx == len(placeholders) - 1 else ".+?"
        pattern_parts.append(f"(?P<{name}>{quantifier})")
        last_index = placeholder.end()

    pattern_parts.append(re.escape(mask[last_index:]))
    pattern = "".join(pattern_parts)
    return re.compile(rf"^{pattern}$")


def _convert_lines(lines: Iterable[str], input_mask: str, output_mask: str) -> List[str]:
    regex = _mask_to_regex(input_mask)
    required_fields = set(_extract_placeholders(output_mask))
    converted: List[str] = []

    for raw_line in lines:
        line = raw_line.strip()
        if not line:
            continue

        match = regex.match(line)
        if not match:
            continue

        values = match.groupdict()

        missing = required_fields.difference(values)
        if missing:
            raise ValueError(
                "–í –≤—ã—Ö–æ–¥–Ω–æ–π –º–∞—Å–∫–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è: " + ", ".join(sorted(missing))
            )

        converted.append(output_mask.format_map(values))

    return converted


def _strip_trailing_punctuation(url: str) -> str:
    return url.rstrip(".,)>]}\n\r")


def _filename_from_headers(headers: Optional[Mapping[str, str]]) -> Optional[str]:
    content_disposition = headers.get("Content-Disposition") if headers else None
    if not content_disposition:
        return None

    match = re.search(r"filename\*=UTF-8''([^;]+)", content_disposition)
    if match:
        return unquote(match.group(1))

    match = re.search(r'filename="?([^";]+)"?', content_disposition)
    if match:
        return match.group(1)

    return None


def _extract_drive_file_id(parsed_url) -> Optional[str]:
    if parsed_url.path.startswith("/file/d/"):
        parts = parsed_url.path.split("/")
        try:
            file_index = parts.index("d") + 1
            return parts[file_index]
        except (ValueError, IndexError):
            return None

    query = parse_qs(parsed_url.query)
    for key in ("id", "fid"):
        if key in query and query[key]:
            return query[key][0]

    path_parts = parsed_url.path.split("/")
    if "uc" in path_parts:
        try:
            idx = path_parts.index("uc")
            if idx + 1 < len(path_parts):
                return path_parts[idx + 1]
        except ValueError:
            pass

    return None



def _extract_drive_folder_id(parsed_url) -> Optional[str]:
    parts = [part for part in parsed_url.path.split("/") if part]
    if "folders" in parts:
        idx = parts.index("folders")
        if idx + 1 < len(parts):
            return parts[idx + 1]

    return None



async def _download_google_drive_file(
    session: aiohttp.ClientSession, file_id: str
) -> Tuple[bytes, Optional[str]]:
    download_url = "https://drive.google.com/uc"
    params = {"export": "download", "id": file_id}

    async with session.get(download_url, params=params) as response:
        response.raise_for_status()
        content_type = response.headers.get("Content-Type", "")

        if "text/html" not in content_type:
            content = await response.read()
            filename = _filename_from_headers(response.headers)
            return content, filename or f"google-drive-{file_id}"

        page = await response.text()
        token_match = re.search(r"confirm=([0-9A-Za-z_]+)", page)
        if not token_match:
            raise ValueError(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª –∏–∑ Google Drive. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –¥–æ—Å—Ç—É–ø –ø–æ —Å—Å—ã–ª–∫–µ –æ—Ç–∫—Ä—ã—Ç."
            )

        params["confirm"] = token_match.group(1)

    async with session.get(download_url, params=params) as response:
        response.raise_for_status()
        content = await response.read()
        filename = _filename_from_headers(response.headers)

    return content, filename or f"google-drive-{file_id}"



async def _download_google_drive_folder(
    folder_id: str, source_url: Optional[str] = None
) -> List[Tuple[bytes, Optional[str]]]:
    loop = asyncio.get_running_loop()

    with tempfile.TemporaryDirectory() as tmpdir:

        def _download_folder() -> List[str]:
            attempts: List[Mapping[str, Any]] = [{"id": folder_id}]
            if source_url:
                attempts.append({"url": source_url})

            last_error: Optional[Exception] = None

            for params in attempts:
                try:
                    result = gdown.download_folder(
                        output=tmpdir,
                        quiet=True,
                        use_cookies=True,
                        remaining_ok=True,
                        **params,
                    )
                except Exception as exc:  # noqa: BLE001
                    last_error = exc
                    continue

                if result:
                    return result

                last_error = last_error or RuntimeError("empty response")

            if last_error:
                raise last_error

            return []

        try:
            downloaded_paths = await loop.run_in_executor(None, _download_folder)
        except Exception as exc:  # noqa: BLE001
            raise ValueError(
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ Google Drive. –ü—Ä–æ–≤–µ—Ä—å –¥–æ—Å—Ç—É–ø –ø–æ —Å—Å—ã–ª–∫–µ."
            ) from exc

        files: List[Tuple[bytes, Optional[str]]] = []
        for raw_path in downloaded_paths:
            path = Path(raw_path)
            if not path.is_file():
                continue
            try:
                relative_path = path.relative_to(tmpdir)
                relative_name = relative_path.as_posix()
            except ValueError:
                relative_name = path.name
            files.append((path.read_bytes(), relative_name))

        if not files:
            raise ValueError(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ Google Drive. –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –¥–æ—Å—Ç—É–ø –æ—Ç–∫—Ä—ã—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞."
            )

        return files


async def _download_yandex_disk_file(
    session: aiohttp.ClientSession,
    public_key: str,
    path: Optional[str] = None,
    suggested_name: Optional[str] = None,
) -> Tuple[bytes, Optional[str]]:
    api_url = "https://cloud-api.yandex.net/v1/disk/public/resources/download"

    params = {"public_key": public_key}
    if path:
        params["path"] = path

    async with session.get(api_url, params=params) as meta_response:
        meta_response.raise_for_status()
        meta = await meta_response.json()

    href = meta.get("href")
    if not href:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–∞.")

    async with session.get(href) as file_response:
        file_response.raise_for_status()
        content = await file_response.read()
        filename = _filename_from_headers(file_response.headers) or suggested_name


    return content, filename



async def _fetch_yandex_resource_meta(
    session: aiohttp.ClientSession,
    public_key: str,
    path: Optional[str] = None,
    offset: Optional[int] = None,
) -> Mapping[str, Any]:
    api_url = "https://cloud-api.yandex.net/v1/disk/public/resources"
    params: dict[str, Any] = {"public_key": public_key, "limit": 1000}
    if path:
        params["path"] = path
    if offset is not None:
        params["offset"] = offset

    async with session.get(api_url, params=params) as response:
        response.raise_for_status()
        return await response.json()


async def _download_yandex_disk_folder(
    session: aiohttp.ClientSession,
    public_key: str,
    root_meta: Mapping[str, Any],
) -> List[Tuple[bytes, Optional[str]]]:
    results: List[Tuple[bytes, Optional[str]]] = []

    async def _walk(path: Optional[str], prefix: Path, meta: Mapping[str, Any]) -> None:
        embedded = meta.get("_embedded", {}) if isinstance(meta, Mapping) else {}
        items = embedded.get("items", []) if isinstance(embedded, Mapping) else []

        if not items and prefix == Path():
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–µ.")

        async def _process_items(current_items: Iterable[Mapping[str, Any]]) -> None:
            for item in current_items:
                if not isinstance(item, Mapping):
                    continue
                item_type = item.get("type")
                item_name = item.get("name") or "file"
                item_path = item.get("path")
                relative = (prefix / item_name).as_posix()

                if item_type == "file":
                    content, _ = await _download_yandex_disk_file(
                        session,
                        public_key,
                        path=item_path,
                        suggested_name=item_name,
                    )
                    results.append((content, relative))
                elif item_type == "dir":
                    child_meta = await _fetch_yandex_resource_meta(
                        session, public_key, path=item_path
                    )
                    await _walk(item_path, Path(relative), child_meta)

        await _process_items(items)

        embedded_meta = embedded if isinstance(embedded, Mapping) else {}
        total = embedded_meta.get("total", len(items)) or 0
        limit = embedded_meta.get("limit", len(items)) or len(items)
        if limit <= 0:
            limit = len(items) or 1
        offset = embedded_meta.get("offset", 0) or 0

        while offset + limit < total:
            offset += limit
            next_meta = await _fetch_yandex_resource_meta(
                session, public_key, path=path, offset=offset
            )
            next_embedded = (
                next_meta.get("_embedded", {}) if isinstance(next_meta, Mapping) else {}
            )
            next_items = next_embedded.get("items", []) if isinstance(next_embedded, Mapping) else []
            limit = next_embedded.get("limit", len(next_items)) or len(next_items)
            if limit <= 0:
                limit = len(next_items) or 1
            offset = next_embedded.get("offset", offset) or offset
            await _process_items(next_items)

    await _walk(None, Path(), root_meta)

    if not results:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–µ.")

    return results


async def _download_files_from_link(url: str) -> List[Tuple[bytes, Optional[str]]]:
    parsed = urlparse(url)
    if parsed.scheme not in {"http", "https"}:
        raise ValueError("–ü–æ—Ö–æ–∂–µ, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –∏–º–µ–µ—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç.")

    domain = parsed.netloc.lower()


    if "drive.google.com" in domain:
        folder_id = _extract_drive_folder_id(parsed)
        if folder_id:
            return await _download_google_drive_folder(folder_id, url)

    async with aiohttp.ClientSession() as session:
        if "drive.google.com" in domain:
            file_id = _extract_drive_file_id(parsed)
            query_params = parse_qs(parsed.query)
            folder_id_fallback = query_params.get("id", [None])[0]

            if file_id:
                try:
                    content, filename = await _download_google_drive_file(session, file_id)
                    return [(content, filename)]
                except ValueError:
                    if folder_id_fallback and folder_id_fallback == file_id:
                        return await _download_google_drive_folder(folder_id_fallback, url)
                    raise

            if folder_id_fallback:
                return await _download_google_drive_folder(folder_id_fallback, url)

            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞ Google Drive.")

        if any(domain.endswith(part) for part in ("yadi.sk", "disk.yandex.ru")):
            root_meta = await _fetch_yandex_resource_meta(session, url)
            resource_type = root_meta.get("type") if isinstance(root_meta, Mapping) else None

            if resource_type == "file":
                content, filename = await _download_yandex_disk_file(
                    session, url, suggested_name=root_meta.get("name") if isinstance(root_meta, Mapping) else None
                )
                return [(content, filename)]

            if resource_type == "dir":
                return await _download_yandex_disk_folder(session, url, root_meta)

            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ä–µ—Å—É—Ä—Å–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–∞ –ø–æ —Å—Å—ã–ª–∫–µ.")


    raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ Google Drive –∏ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫.")


async def handle_start(message: Message, state: FSMContext) -> None:
    await state.clear()
    await _answer_with_retry(
        message,
        "üëã –ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –º–∞—Å–∫—É —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–æ–∫ —Ñ–∞–π–ª–∞.\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
        "`{protocol}://{domain}:{username}:{password}`",
        parse_mode=ParseMode.MARKDOWN,
    )
    await state.set_state(ConversionStates.input_mask)


async def handle_cancel(message: Message, state: FSMContext) -> None:
    await state.clear()
    await _answer_with_retry(
        message,
        "–°–±—Ä–æ—Å–∏–ª —Ç–µ–∫—É—â—É—é –æ–ø–µ—Ä–∞—Ü–∏—é. –ù–∞–ø–∏—à–∏ /start, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ.",
    )


async def handle_input_mask(message: Message, state: FSMContext) -> None:
    mask = message.text or ""
    try:
        _mask_to_regex(mask)
    except ValueError as exc:
        await _answer_with_retry(message, str(exc))
        return

    await state.update_data(input_mask=mask)
    await state.set_state(ConversionStates.output_mask)
    await _answer_with_retry(
        message,
        "–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤—å –º–∞—Å–∫—É –Ω—É–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ –∂–µ –∏–º–µ–Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä: `{username}:{password}`",
        parse_mode=ParseMode.MARKDOWN,
    )


async def handle_output_mask(message: Message, state: FSMContext) -> None:
    mask = message.text or ""
    if not PLACEHOLDER_PATTERN.search(mask):
        await _answer_with_retry(
            message,
            "–í –º–∞—Å–∫–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä –≤ —Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö.",
        )
        return

    await state.update_data(output_mask=mask)

    await state.set_state(ConversionStates.lines_per_file)
    await _answer_with_retry(
        message,
        "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤, –≤–≤–µ–¥–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω —Ñ–∞–π–ª.\n"
        "–û—Ç–ø—Ä–∞–≤—å 0 –∏–ª–∏ —Å–ª–æ–≤–æ 'skip', —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª.",
    )


async def handle_lines_per_file(message: Message, state: FSMContext) -> None:
    text = (message.text or "").strip().lower()

    if not text:
        await _answer_with_retry(
            message,
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –∏–ª–∏ 0, —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å.",
        )
        return

    if text in {"skip", "0", "–Ω–µ—Ç", "no"}:
        lines_per_file = 0
    else:
        try:
            lines_per_file = int(text)
        except ValueError:
            await _answer_with_retry(
                message,
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —á–∏—Å–ª–æ. –ü–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞.",
            )
            return

        if lines_per_file <= 0:
            lines_per_file = 0

    await state.update_data(lines_per_file=lines_per_file)
    await state.set_state(ConversionStates.waiting_file)
    await _answer_with_retry(
        message,
        "–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –ø—Ä–∏—à–ª–∏ —Ñ–∞–π–ª –≤ –≤–∏–¥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª/–ø–∞–ø–∫—É. "
        "–Ø –≤–µ—Ä–Ω—É –µ–≥–æ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.",
    )




def _output_stem_from_source(source_name: Optional[str]) -> str:
    if not source_name:
        return "converted"

    candidate = source_name.replace("\\", "/").split("/")[-1]
    stem = Path(candidate).stem
    sanitized = re.sub(r"[^A-Za-z0-9_-]+", "_", stem)
    return sanitized or "converted"


def _output_filename(stem: str, part_index: int, total_parts: int) -> str:
    if total_parts <= 1:
        return f"{stem}.txt"
    return f"{stem}_part_{part_index}.txt"

def _zip_entries(entries: Iterable[Tuple[str, bytes]]) -> bytes:
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
        for relative_path, output_bytes in entries:
            zip_file.writestr(relative_path, output_bytes)
    return buffer.getvalue()


def _split_entries_into_archives(
    entries: List[Tuple[str, bytes]],
    archive_stem: str,
) -> Tuple[List[Tuple[str, bytes]], Optional[Tuple[str, int]]]:
    for relative_path, output_bytes in entries:
        if len(output_bytes) > MAX_TELEGRAM_FILE_SIZE:
            return [], (relative_path, len(output_bytes))

    archives: List[Tuple[str, bytes]] = []
    if not entries:
        return archives, None

    current_chunk: List[Tuple[str, bytes]] = []
    current_zip: Optional[bytes] = None

    for relative_path, output_bytes in entries:
        current_chunk.append((relative_path, output_bytes))
        current_zip = _zip_entries(current_chunk)

        if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
            current_chunk.pop()

            if current_chunk:
                archives.append(("", _zip_entries(current_chunk)))
            else:
                return [], (relative_path, len(output_bytes))

            current_chunk = [(relative_path, output_bytes)]
            current_zip = _zip_entries(current_chunk)

            if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
                return [], (relative_path, len(output_bytes))

    if current_chunk:
        assert current_zip is not None
        if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
            single_path, single_bytes = current_chunk[0]
            return [], (single_path, len(single_bytes))
        archives.append(("", current_zip))

    total_parts = len(archives)
    named_archives: List[Tuple[str, bytes]] = []
    for index, (_, archive_bytes) in enumerate(archives, start=1):
        if total_parts == 1:
            archive_name = f"{archive_stem}.zip"
        else:
            archive_name = f"{archive_stem}_part_{index}.zip"
        named_archives.append((archive_name, archive_bytes))
        
def _zip_entries(entries: Iterable[Tuple[str, bytes]]) -> bytes:
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
        for relative_path, output_bytes in entries:
            zip_file.writestr(relative_path, output_bytes)
    return buffer.getvalue()


def _split_entries_into_archives(
    entries: List[Tuple[str, bytes]],
    archive_stem: str,
) -> Tuple[List[Tuple[str, bytes]], Optional[Tuple[str, int]]]:
    for relative_path, output_bytes in entries:
        if len(output_bytes) > MAX_TELEGRAM_FILE_SIZE:
            return [], (relative_path, len(output_bytes))

    archives: List[Tuple[str, bytes]] = []
    if not entries:
        return archives, None

    current_chunk: List[Tuple[str, bytes]] = []
    current_zip: Optional[bytes] = None

    for relative_path, output_bytes in entries:
        current_chunk.append((relative_path, output_bytes))
        current_zip = _zip_entries(current_chunk)

        if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
            current_chunk.pop()

            if current_chunk:
                archives.append(("", _zip_entries(current_chunk)))
            else:
                return [], (relative_path, len(output_bytes))

            current_chunk = [(relative_path, output_bytes)]
            current_zip = _zip_entries(current_chunk)

            if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
                return [], (relative_path, len(output_bytes))

    if current_chunk:
        assert current_zip is not None
        if len(current_zip) > MAX_TELEGRAM_FILE_SIZE:
            single_path, single_bytes = current_chunk[0]
            return [], (single_path, len(single_bytes))
        archives.append(("", current_zip))

    total_parts = len(archives)
    named_archives: List[Tuple[str, bytes]] = []
    for index, (_, archive_bytes) in enumerate(archives, start=1):
        if total_parts == 1:
            archive_name = f"{archive_stem}.zip"
        else:
            archive_name = f"{archive_stem}_part_{index}.zip"
        named_archives.append((archive_name, archive_bytes))

    return named_archives, None



async def _convert_file_to_outputs(
    message: Message,
    state: FSMContext,
    file_content: bytes,
    source_name: Optional[str] = None,
) -> Optional[Tuple[str, List[Tuple[str, bytes]]]]:

    data = await state.get_data()
    input_mask = data.get("input_mask")
    output_mask = data.get("output_mask")

    lines_per_file = int(data.get("lines_per_file", 0) or 0)

    if not input_mask or not output_mask:
        await _answer_with_retry(
            message,
            "–°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å –º–∞—Å–∫–∏ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã /start.",
        )

        return None


    text = file_content.decode("utf-8", errors="ignore")
    try:
        converted_lines = _convert_lines(text.splitlines(), input_mask, output_mask)
    except ValueError as exc:
        await _answer_with_retry(
            message,
            f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ñ–∞–π–ª: {exc}",
        )
        return None

    if not converted_lines:
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç—Ä–æ–∫–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–æ–¥ –≤—Ö–æ–¥–Ω—É—é –º–∞—Å–∫—É.",
        )

        return None

    if lines_per_file > 0:
        chunks = [
            converted_lines[idx : idx + lines_per_file]
            for idx in range(0, len(converted_lines), lines_per_file)
        ]
    else:
        chunks = [converted_lines]

    if len(chunks) > 1:
        details = f" –¥–ª—è {source_name}" if source_name else ""
        await _answer_with_retry(
            message,
            f"–ì–æ—Ç–æ–≤–æ! –†–∞–∑–±–∏–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ {len(chunks)} —Ñ–∞–π–ª–∞(–æ–≤){details}.",
        )

    stem = _output_stem_from_source(source_name)
    outputs: List[Tuple[str, bytes]] = []

    for idx, chunk in enumerate(chunks, start=1):
        output_bytes = "\n".join(chunk).encode("utf-8")
        filename = _output_filename(stem, idx, len(chunks))
        outputs.append((filename, output_bytes))

    return stem, outputs


async def handle_file(message: Message, state: FSMContext, bot: Bot) -> None:
    document = message.document
    if not document:
        await _answer_with_retry(
            message,
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å —Ñ–∞–π–ª –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç (–Ω–µ –∫–∞–∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é).",
        )
        return

    file = await bot.get_file(document.file_id)
    buffer = io.BytesIO()
    await bot.download_file(file.file_path, buffer)
    buffer.seek(0)

    result = await _convert_file_to_outputs(
        message,
        state,
        buffer.read(),
        source_name=document.file_name,
    )

    if not result:
        return

    stem, outputs = result

    if len(outputs) == 1:
        filename, output_bytes = outputs[0]
        caption = (
            f"–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è {document.file_name}."
            if document.file_name
            else "–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª."
        )
        if not await _send_document(message, output_bytes, filename, caption):
            return
    else:
        caption_base = (

            f"–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è {document.file_name}."
            if document.file_name
            else "–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."
        )

        archives, oversized = _split_entries_into_archives(outputs, stem)
        if oversized:
            filename, size = oversized
            size_mb = size / (1024 * 1024)
            await _answer_with_retry(
                message,
                "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª "
                f"{filename}, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {size_mb:.1f} –ú–ë. "
                "–ü–æ–ø—Ä–æ–±—É–π —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–º–µ–Ω—å—à–∏–≤ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.",

            )
            return

        if not archives:
            await _answer_with_retry(
                message,
                "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∞—Ä—Ö–∏–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.",
            )
            return

        if len(archives) > 1:
            await _answer_with_retry(
                message,
                "–ò—Ç–æ–≥–æ–≤—ã–π –∞—Ä—Ö–∏–≤ –ø–æ–ª—É—á–∏–ª—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º, –ø–æ—ç—Ç–æ–º—É —Ä–∞–∑–±–∏–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ "
                f"{len(archives)} —á–∞—Å—Ç–∏.",

            )

        total_parts = len(archives)
        for index, (archive_name, archive_bytes) in enumerate(archives, start=1):
            caption = caption_base
            if total_parts > 1:
                caption = f"{caption_base} –ß–∞—Å—Ç—å {index} –∏–∑ {total_parts}."
            if index > 1:
                await asyncio.sleep(1)
            if not await _send_document(message, archive_bytes, archive_name, caption):
                return

        await _answer_with_retry(message, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


    await state.clear()


async def handle_file_link(message: Message, state: FSMContext) -> None:
    text = (message.text or "").strip()
    match = URL_PATTERN.search(text)
    if not match:
        await _answer_with_retry(
            message,
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª/–ø–∞–ø–∫—É –≤ Google Drive –∏–ª–∏ –Ω–∞ –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–µ.",
        )
        return

    url = _strip_trailing_punctuation(match.group(0))

    try:
        files = await _download_files_from_link(url)
    except ValueError as exc:
        await _answer_with_retry(message, str(exc))
        return
    except aiohttp.ClientError:
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –ø–æ —Å—Å—ã–ª–∫–µ. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å –¥–æ–∫—É–º–µ–Ω—Ç.",
        )
        return

    if not files:
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª—ã –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–µ.",
        )
        return

    if len(files) > 1:
        await _answer_with_retry(
            message,
            f"–ù–∞—à—ë–ª {len(files)} —Ñ–∞–π–ª–∞(–æ–≤) –ø–æ —Å—Å—ã–ª–∫–µ. –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É...",
        )

    converted_entries: List[Tuple[str, bytes]] = []
    archive_stem: Optional[str] = None
    single_source_name: Optional[str] = None
    success_any = False

    for content, name in sorted(files, key=lambda item: item[1] or ""):
        result = await _convert_file_to_outputs(message, state, content, source_name=name)
        if not result:
            continue

        success_any = True
        stem, outputs = result
        if len(files) == 1:
            archive_stem = stem
            single_source_name = name

        for filename, output_bytes in outputs:
            relative_path = filename
            if len(files) > 1:
                relative_path = f"{stem}/{filename}"
            converted_entries.append((relative_path, output_bytes))

    if not success_any:
        return

    await state.clear()

    if not converted_entries:
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∞–π–ª—ã –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.",
        )

        return

    if len(converted_entries) == 1 and len(files) == 1:
        filename, output_bytes = converted_entries[0]
        caption = (
            f"–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è {single_source_name}."
            if single_source_name
            else "–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª."
        )

        await _send_document(message, output_bytes, filename, caption)
        return

    archive_stem = "converted_files" if len(files) > 1 else (archive_stem or "converted")

    caption_base = "–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."
    if len(files) > 1:
        caption_base = (
            "–ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–ª –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ —Å—Å—ã–ª–∫–µ. –í–æ—Ç –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."
        )
    elif single_source_name:
        caption_base = f"–ì–æ—Ç–æ–≤–æ! –í–æ—Ç –∞—Ä—Ö–∏–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è {single_source_name}."

    archives, oversized = _split_entries_into_archives(converted_entries, archive_stem)
    if oversized:
        filename, size = oversized
        size_mb = size / (1024 * 1024)
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª "
            f"{filename}, –ø–æ—Ç–æ–º—É —á—Ç–æ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {size_mb:.1f} –ú–ë. "
            "–ü–æ–ø—Ä–æ–±—É–π —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–º–µ–Ω—å—à–∏–≤ —á–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.",

        )
        return

    if not archives:
        await _answer_with_retry(
            message,
            "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∞—Ä—Ö–∏–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.",
        )
        return

    if len(archives) > 1:
        await _answer_with_retry(
            message,
            "–ò—Ç–æ–≥–æ–≤—ã–π –∞—Ä—Ö–∏–≤ –ø–æ–ª—É—á–∏–ª—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º, –ø–æ—ç—Ç–æ–º—É —Ä–∞–∑–±–∏–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ "
            f"{len(archives)} —á–∞—Å—Ç–∏.",
        )

    total_parts = len(archives)
    for index, (archive_name, archive_bytes) in enumerate(archives, start=1):
        caption = caption_base
        if total_parts > 1:
            caption = f"{caption_base} –ß–∞—Å—Ç—å {index} –∏–∑ {total_parts}."
        if index > 1:
            await asyncio.sleep(1)
        if not await _send_document(message, archive_bytes, archive_name, caption):
            return

    await _answer_with_retry(message, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


    total_parts = len(archives)
    for index, (archive_name, archive_bytes) in enumerate(archives, start=1):
        caption = caption_base
        if total_parts > 1:
            caption = f"{caption_base} –ß–∞—Å—Ç—å {index} –∏–∑ {total_parts}."
        if index > 1:
            await asyncio.sleep(1)
        if not await _send_document(message, archive_bytes, archive_name, caption):
            return

    await _answer_with_retry(message, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")


    try:
        await message.answer_document(
            BufferedInputFile(archive_buffer.getvalue(), filename=archive_name),
            caption=caption,
        )
    except TelegramBadRequest:
        await message.answer(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª. –ü–æ–ø—Ä–æ–±—É–π —Ñ–∞–π–ª –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."
        )

async def main() -> None:
    load_dotenv()
    token = os.environ.get("BOT_TOKEN")
    if not token:
        raise RuntimeError("–£–∫–∞–∂–∏—Ç–µ —Ç–æ–∫–µ–Ω –±–æ—Ç–∞ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è BOT_TOKEN")

    logging.basicConfig(level=logging.INFO)

    bot = Bot(token=token, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
    dp = Dispatcher()

    dp.message.register(handle_start, CommandStart())
    dp.message.register(handle_cancel, Command(commands=["cancel"]))
    dp.message.register(handle_file, ConversionStates.waiting_file, F.document)
    dp.message.register(handle_file_link, ConversionStates.waiting_file, F.text)

    dp.message.register(handle_lines_per_file, ConversionStates.lines_per_file, F.text)

    dp.message.register(handle_output_mask, ConversionStates.output_mask, F.text)
    dp.message.register(handle_input_mask, ConversionStates.input_mask, F.text)

    await dp.start_polling(bot)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        pass
